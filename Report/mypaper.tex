% This is based on LLNCS.dem the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
% Modified by Gabriele Bleser, January 2013
% Modified by Oliver Wasenmüller, March 2016
% Modified by Oliver Wasenmüller, November 2017
% 
% This is the Latex main document. If you need additional packages, 
% you can insert them here. Otherwise, you should not modify this.
% Provide your contribution in a separate file, such as mypaper.tex.
% All contributions will be included here.
%

\documentclass{llncs}

% Include your packages here
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{subfiles}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{url}

\usepackage{caption}
\usepackage{cite}
\usepackage{copyrightbox}
\usepackage[bookmarksnumbered=true]{hyperref} 
\hypersetup{
     colorlinks = true,
     linkcolor = red,
     citecolor = green,
     filecolor = red,
     urlcolor = blue
     }


% use this for review submission only. Please comment it for final submission
\usepackage{lineno}
\linenumbers
%

\begin{document}

\pagestyle{headings}  % switches on printing of running heads

%
\title{Survey on Face Tracking with Deep Learning}
%
\titlerunning{short title}  % abbreviated title (for running head)
%
\author{Vinay Balasubramanian\inst{1} \and Jilliam Diaz Barros\inst{2}}
%
\authorrunning{Your name et al.} % abbreviated author list (for running head)
%
\institute{\email{v\_balasubr18@cs.uni-kl.de}
\and
\email{jilliam\_maria.diaz\_barros@dfki.de}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
In this paper, we review different face tracking architectures and their performance in challenging conditions. We focus on deep-learning based methods that exploit the temporal information across frames, i.e video-based methods. Recent developments include using an encoder-decoder network, recurrent network, deep reinforcement learning, and two-stream network. This paper aims to compare those approaches in terms of accuracy, the dataset(s) used for training, evaluation metrics, robustness to large head poses and occlusions, etc
\keywords{Face tracking, Facial landmarks, Deep Learning, Reinforcement Learning, Temporal information}
\end{abstract}


% This is your content. Name the sections appropriately
\section{Introduction}
Face tracking is a computer vision task of tracking the face across all frames of a video. It may involve tracking specific landmarks around the face, or tracking a bounding box around the face across frames.
Face Tracking technology plays an important role in computer vision applications such as \textit{Face recognition}\cite{face_recognition}, \textit{Expression recognition}\cite{expression_recognition} and \textit{Face modeling}\cite{face_modeling}. This is a challenging problem as the videos may not be captured in constrained conditions and may have illumination inconsistencies, large head poses, blurriness, occlusions etc.\\

There are various approaches to this problem. Some of them are image-based methods, where the models are trained on still frames and the detection also happens independently at each frame. Other methods are video-based and use an incremental-learning technique to exploit the temporal connection between successive frames. We make a comparison between the different architectures, datasets used for training and testing, evaluation metrics and robustness to challenging conditions. Figure \ref{generic_video_based} shows a generic high-level architecture of a video-based landmark detection pipeline.

\begin{figure}[h!]

\includegraphics[scale=0.5]{Media/generic_video_based}
\caption{Generic architecture of video based methods. Landmarks detected in the current frame are used as an initialization for the next frame}
\label{generic_video_based}
\end{figure}

\section{Datasets}

In this section, we list the datasets commonly used for landmark-based face tracking. There are many publicly available datasets for face tracking. Datasets can be categorized into constrained datasets and unconstrained datasets (in the wild).
Table \ref{datasets} shows various image-based and video datasets.


\begin{table}[htbp]
\caption{Datasets}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Dataset} & \textbf{Description}                   & \textbf{Contains}                                                                                                                                 & \textbf{Wild?} \\ \hline
300VW\cite{300-VW_1, 300-VW_2, 300-VW_3}            & 300 videos in the wild                 & \begin{tabular}[c]{@{}l@{}}114 videos with 218,595 frames \\with 68  landmarks per frame\end{tabular}                                             & Yes            \\ \hline
AFLW\cite{aflw}             & \begin{tabular}[c]{@{}l@{}}Annotated Facial Landmarks \\in the Wild\end{tabular} & \begin{tabular}[c]{@{}l@{}}Around 25k annotated face \\images with 21  landmarks per image\end{tabular}                                           & Yes            \\ \hline
LFW\cite{lfw}              & Labeled Faces in the Wild              & \begin{tabular}[c]{@{}l@{}}13,233 images of 5749 people detected \\ and centered by Viola Jones face detector\end{tabular}                        & Yes            \\ \hline
Helen\cite{helen}            & Helen facial feature dataset           & \begin{tabular}[c]{@{}l@{}}2000 training and 330 test images with \\194 landmarks and accurate \\ annotations of primary facial components\end{tabular} & Yes             \\ \hline
LFPW\cite{lfpw}             & Labeled Face Parts in the Wild         & \begin{tabular}[c]{@{}l@{}}1432 images with 29 landmarks on each \\ image\end{tabular}                                                            & Yes            \\ \hline
TF\cite{tf}               & Talking Face                           & \begin{tabular}[c]{@{}l@{}}5000 frames of a person engaged in a\\ conversation with 68 landmarks in each \\frame on each image\end{tabular}                                                 & No             \\ \hline
FM\cite{fm}               & Face Movies                            & \begin{tabular}[c]{@{}l@{}}2150 images of 6 videos with 68 \\landmarks on each image\end{tabular}                                                & Yes            \\ \hline
SynHead\cite{dynamic_facial_analysis}          & Synthetic dataset                      & 
\begin{tabular}[c]{@{}l@{}}510,960 frames of 70 head motion tracks                                                                                                \\
that include large face pose variations       \end{tabular} & No            \\ \hline
BIWI\cite{biwi}             & Biwi kinect head pose database         & \begin{tabular}[c]{@{}l@{}}24 videos with over 15k frames of 20\\ people                                                                                                                   \end{tabular}   & Yes            \\ \hline
COFW \cite{cofw}             & \begin{tabular}[c]{@{}l@{}}Caltech Occluded Faces \\in the Wild \end{tabular}    & \begin{tabular}[c]{@{}l@{}}1007 occluded face images with 29 \\manually annotated landmarks\end{tabular}                                         & Yes            \\ \hline
IBUG\cite{ibug}             & IBUG dataset     & \begin{tabular}[c]{@{}l@{}}135 images with difficult poses and \\expressions\end{tabular}                                         & Yes            \\ \hline
RWMB\cite{fab}             & Real-World Motion Blur     & \begin{tabular}[c]{@{}l@{}}10000 faces with 98 landmarks including \\occlusion, blur, illumination changes etc.\end{tabular}                                         & Yes            \\ \hline

\end{tabular}
\label{datasets}
\end{table}

\section{Face Tracking Approaches}

In this section, we look at some of the state-of-the-art approaches for video-based facial landmark tracking. Deep learning methods, in general, use CNN and RNN to detect landmarks.
\subsection{Recurrent Encoder-Decoder Network for Video-based Face Alignment (2016) \cite{rednet}}

This method leverages temporal information to predict facial landmarks in each frame. It uses recurrent learning at both spatial and temporal dimensions. At the temporal level, the features are separated into \textit{temporal-variant} features such as pose and expression, and \textit{temporal-invariant} features such as facial identity and recurrent learning is applied to the temporal-variant features. This feature disentangling has shown to achieve better generalization and more accurate results. Figure \ref{rednet_architecture} shows the pipeline of recurrent encoder-decoder network \\

\begin{figure}
\copyrightbox[b]{\includegraphics[scale=0.5]{Media/rednet_architecture}}%
{Source: Xi Peng, Rogerio S. Feris, Xiaoyu Wang, Dimitris N. Metaxas. RED-Net: A Recurrent Encoder-Decoder Network for Video-based Face Alignment}
\caption{Overview of REDNet pipeline}
\label{rednet_architecture}
\end{figure}

The network consists of 4 modules -
\begin{itemize}
\item[(1)] \textbf{Encoder-Decoder}:-
The encoder encodes features from a single video frame into an intermediate low dimensional representation by performing a sequence of convolutions, pooling and batch normalization. The decoder upsamples the low dimensional representation and transforms it into a response map that contains facial landmarks.\\


\item[(2)] \textbf{Spatial recurrent learning}:-
The purpose is to find the exact location of landmarks in a coarse-to-fine manner by iteratively providing the previous prediction as feedback along with the video frame. This is carried out in 2 steps - \textit{Landmark Detection} and \textit{Landmark Regression}. Landmark detection step locates 7 major facial components whereas landmark regression step refines predicted locations of all 68 landmark positions\\

\item[(3)] \textbf{Temporal recurrent learning}:-
This is proposed to model the temporal-variant factors such as pose and expression. The temporal variations in the temporal-invariant factors (non-identity code) are modeled using an LSTM unit consisting of 256 hidden neurons.
Trained using \textit{T} successive frames. Detection and regression tasks are performed frame by frame. 
\\

\item[(4)] \textbf{Supervised identity disentangling}:-
Complete identity and non-identity factor disentangling cannot be guaranteed. More supervised information is needed to achieve better separation of the features. This module applies identity constraint to the identity code to further separate identity code from the non-identity code. Face recognition is applied to the identity code to classify the people present in the frames. This is shown to yield better generalization and better test accuracy\\
\end{itemize}
 

\subsection{Dynamic Facial Analysis using Recurrent Neural Networks (2017) \cite{dynamic_facial_analysis}}

This approach uses RNN for joint estimation and face tracking. 
It proposes RNN as an alternative approach that performs better than previous video-based approaches for dynamic facial analysis which use Kalman filters or particle filters, inspired by the fact that RNNs and Bayesian filters are operationally very similar.
Bayesian filters need problem-specific hand-tuning. Given sufficient data, an RNN can be trained to do the same task and avoid problem-specific tracker engineering. The head pose is estimated in terms of pitch, yaw and roll angles
The authors create a synthetic dataset \textbf{SynHead} to cater to the need for large training data. The approach employs FC-RNN to exploit the generalization from a pre-trained CNN. It consists of CNN layers followed by recurrent layers as dense layers. RNN is more robust to occlusions and large head poses. Figure \ref{cnn_rnn} shows the proposed end-to-end network for joint estimation and tracking. The CNN and RNN are trained together end-to-end. The network is a modified VGG16 with an extra fully connected layer with 1024 neurons and the output layer consists of 3 neurons for the pitch, yaw and roll angles. For facial landmark detection, the same network is used with the only difference that the output layer contains 136 neurons corresponding to the locations of the 68 landmarks.\\


For each frame, the mean Euclidean distance of the 68 landmarks normalized by the diagonal distance of the ground truth box is computed. The metrics used for evaluation are \textit{area under the curve} which is the area under the cumulative error distribution curve, and \textit{failure rate} which is the percentage of images whose errors are larger than a given threshold.

\begin{figure}
\copyrightbox[b]{\includegraphics[scale=0.5]{Media/cnn_rnn}}%
{Source: Jinwei Gu, Xiaodong Yang, Shalini De Mello, Jan Kautz. Dynamic Facial Analysis: From Bayesian Filters to Recurrent Neural Network}
\caption{Proposed end-to-end CNN RNN network}
\label{cnn_rnn}
\end{figure}




\subsection{Dual-Agent Deep Reinforcement Learning (2018) \cite{dadrl}}

This approach exploits the fact that bounding box tracking and landmark detection are dependent. The accuracy of facial landmarks detected depends on how good the bounding box is.
Figure \ref{deformable_face_tracking} shows different strategies for deformable face tracking. This paper proposes DADRL (Dual-Agent Deep Learning) framework for simultaneous bounding box tracking and landmark detection in an interactive manner. It uses reinforcement learning to learn to make adaptive decisions during face tracking. The architecture consists of a \textit{Tracking agent} and an \textit{Alignment agent} and \textit{communication channels} between the agents. The two agents are trained simultaneously to learn two conditional distributions. Figure \ref{dadrl_architecture} shows the proposed architecture. The message channels are trained using deep Q-learning algorithm\\

\begin{figure}
\copyrightbox[b]{\includegraphics[scale=0.5]{Media/deformable_face_tracking}}%
{Source: Minghao Guo, Jiwen Lu, Jie Zhou. Dual-Agent Deep Reinforcement Learning for Deformable Face Tracking}
\caption{Strategies for deformable face tracking}
\label{deformable_face_tracking}
\end{figure}

If $I_k$ is the $k^{th}$ frame, $B_k$ is the bounding box for the $k^{th}$ frame and $V_k$ is the vector of $L$ landmarks, then by probabilistic duality -  

$p\left(B_{k} | I_{k}\right) p\left(V_{k} | B_{k}, I_{k}\right)=p\left(V_{k} | I_{k}\right) p\left(B_{k} | V_{k}, I_{k}\right)$\\

\begin{figure}
\copyrightbox[b]{\includegraphics[scale=0.5]{Media/DADRL}}%
{Source: Minghao Guo, Jiwen Lu, Jie Zhou. Dual-Agent Deep Reinforcement Learning for Deformable Face Tracking}
\caption{DADRL architecture}
\label{dadrl_architecture}
\end{figure}

The learning objectives of bounding box tracking and landmark detection are treated as two conditional probabilities and the dependency between these two tasks is formulated as two marginal distributions. Since the ground-truth marginal distributions are not available, communication channels between the agents are used as alternatives to satisfy the probabilistic duality. For each frame, the terminal state of the previous frame is used for initializing the current state. The two agents decide a sequence of actions based on the observed state and exchanged messages, to adjust the bounding box and regress facial landmarks simultaneously. The messages sent from the tracking agent to the alignment agent are encoded by a deconvolution layer. It provides additional textural information to the alignment agent to improve its robustness. The messages from the alignment agent to the tracking agent are encoded by an LSTM unit. It provides 3D pose information to the tracking agent to improve bounding box tracking.\\


\subsection{Two Stream Transformer Networks (2017) \cite{tstn}}

This approach aims to capture both spatial information on still frames as well as temporal information across frames. It proposes a two-stream deep learning method that decomposes the video input to spatial and temporal streams. The spatial stream aims to capture appearance information from still frames. It is trained to transform image pixels to landmark positions directly on still frames and then to refine the current facial shape based on the previous shape. The temporal stream aims to capture temporal consistency information across successive frames. It consists of an encoder-decoder module. The encoder is trained to encode the spatial information as active appearance codes that capture the whole face changes across frames in the temporal dimension. The decoder remaps the learned codes to the original face input size. The temporal consistency information for each landmark is used to improve alignment accuracy. It also consists of a two-layer RNN in between the encoder-decoder module. The first layer captures spatial-temporal appearance features whereas the second layer memorizes the temporal information across frames. Facial landmarks are determined by a weighted fusion of both spatial and temporal streams. Figure \ref{TSTN} shows the proposed architecture. The landmark positions are refined simultaneously in both the streams.

\begin{figure}
\copyrightbox[b]{\includegraphics[scale=0.5]{Media/TSTN}}%
{Source: Hao Liu, Senior Member, IEEE, Jianjiang Feng, Member, IEEE, Jie Zhou, Senior Member, IEEE. Two-Stream Transformer Networks for Video-based Face Alignment}
\caption{TSTN pipeline}
\label{TSTN}
\end{figure}




\subsection{Face Alignment Recurrent Network (2017) \cite{farn}}

Previous state-of-the-art regression-based approaches start with an initial shape estimation and iteratively estimate the facial shape at successive stages by estimating an increment from the previous estimation. This paper proposes to improve the cascade shape regression by using LSTM and Region Convolutional Neural Network (RCNN). The LSTM model exploits both spatial and temporal information for landmark detection in images and videos in uncontrolled conditions. The predicted landmark location is used as a basis for estimation in the next stage (spatial) and used as a basis for estimation in the next frame (temporal). The process continues recurrently until the face shape is finalized. 
Figure \ref{farn_training} shows the training architecture of Face Alignment Recurrent Network. The face image, initial face shape, and ground truth shape are given as input to the network. The image is passed through several convolutional and max-pooling layers to obtain a feature map. The initial face shape contains facial landmarks. Region of Interest (ROI) pooling is applied around each landmark to obtain ROI pooling features. The ROI pooling features are concatenated and given to a fully connected layer followed by an LSTM layer. The network outputs the predicted shape increment over the initial face shape. The initial face shape is summed over the predicted shape increment to obtain updated initial face shape. This process continues recurrently for $T$ stages.

\begin{figure}
\copyrightbox[b]{\includegraphics[scale=0.5]{Media/farn_training}}%
{Source: Hao Liu, Senior Member, IEEE, Jianjiang Feng, Member, IEEE, Jie Zhou, Senior Member, IEEE. Two-Stream Transformer Networks for Video-based Face Alignment}
\caption{FARN training architecture}
\label{farn_training}
\end{figure}



\section{Performance Comparison}
\cite{rednet} is trained on both image and video datasets with different configurations for different datasets. The training happens in 3 steps. In the first step, the network without the temporal recurrent learning and supervised identity disentangling modules is pre-trained using the image datasets AFLW, Helen and LFPW. In the second step, supervised identity disentangling is included and trained with other modules using the image-based LFW dataset. In the third step, the temporal recurrent learning module is included and the entire model is fine-tuned using the video dataset 300-VW. Inter-ocular distance is used to normalize the Root Mean Square Error(RMSE). \cite{dynamic_facial_analysis} is trained using the created SynHead dataset with the L2 loss function and tested on the BIWI dataset. It is then fine-tuned using training data from the BIWI dataset. For landmark detection, the corresponding model is trained and tested using a randomly split 300-VW dataset. \cite{dadrl} is trained in two stages. The \textbf{first stage} is the \textit{supervised learning stage} in which the two agents are trained separately. All training data from 300 faces in the wild challenge (300-W image dataset) is used to train the alignment agent. The 300-VW training set is used to train the tracking agent. The communicated messages are set to zero in this stage. The \textbf{second stage} is the \textit{reinforcement learning stage} in which the whole network is trained with the 300-VW training set. The model is evaluated on the test set of 300-VW. For evaluation, Normalized Root Mean Square Error and cumulative error distribution plots are used. \cite{tstn} is trained using the 300-VW training set. The pre-trained spatial stream network is finetuned beforehand. The model is evaluated on the testing sets of Talking Face(TF) and 300-VW datasets. Normalized Root-Mean-Square-Error and cumulative error distribution plots are used for evaluating the model. \cite{farn} is trained on the training partition consisting of training sets of LFPW, Helen and the entire AFW with 3148 images in total. The testing partition contains 3 parts - the common subset, the challenging subset, and the full set. The common subset consists of testing set of LFPW and Helen with 554 images in total. The challenging subset consists of the IBUG dataset which contains additional annotations for 135 images in difficult poses and expressions. The full set consists of both the common subset and the challenging subset with 689 images. The model is evaluated using point-to-point Root Mean Square Error between the face shape and the ground truth annotations.\\

\cite{rednet}, \cite{dynamic_facial_analysis}, \cite{tstn} and \cite{farn} provide testing results on challenging category of 300-VW test set for 68 landmarks. \cite{rednet} and \cite{tstn} provide results on Talking Face dataset \cite{tf}. \cite{rednet} provides results for both 68 and 7 landmarks in both datasets.
\cite{dadrl} uses normalized point-to-point error for evaluation and hence cannot be compared with the other methods.
Table \ref{comparison} reports the RMSE of the compared methods on 300-VW and TF\cite{tf} datasets.\\
 
\begin{table}[]
\caption{Evaluation on 300-VW and TF test sets}
\begin{tabular}{|l|l|l|l|l|}
\hline
                        & \multicolumn{2}{c|}{\textbf{300-VW}}                                                                                                           & \multicolumn{2}{c|}{\textbf{TF}}                                                                                                               \\ \hline
\textbf{Method}         & \textbf{\begin{tabular}[c]{@{}l@{}}RMSE(68 \\ landmarks)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}RMSE(7 \\ landmarks)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}RMSE(68 \\ landmarks)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}RMSE(7 \\ landmarks)\end{tabular}} \\ \hline
REDNet\cite{rednet}                  & \textbf{5.15}                                                          & \textbf{5.29}                                                         & \textbf{2.77                                                                 }  & 2.89                                                                  \\ \hline
Dynamic Facial Analysis\cite{dynamic_facial_analysis} & 6.16                                                                   &                                                                       &                                                                        &                                                                       \\ \hline
DADRL\cite{dadrl}                   &                                                                        &                                                                       &                                                                        &                                                                       \\ \hline
TSTN\cite{tstn}                    & 5.52                                                                   &                                                                       &                                                                        & \textbf{2.13}                                                         \\ \hline
FARN\cite{farn}                    & 5.49                                                                   &                                                                       &                                                                        &                                                                       \\ \hline
\end{tabular}
\label{comparison}
\end{table}



\section{Conclusion and Discussion}

In this paper, we have reviewed some of the state-of-the-art deep learning methods for video-based face alignment. All of these methods avoid hand-engineering by using neural networks. Each of the methods is independent and not an improvisation of the other. All these methods use RNN in common to model temporal information. \cite{rednet} seems to perform the best on the 300-VW dataset for both 7 and 68 landmarks. \cite{tstn} has the least error on the TF dataset for 7 landmarks. 

%

%
% ---- Bibliography ----
%
\bibliographystyle{plain}
\bibliography{bibexample}
%
\end{document}	